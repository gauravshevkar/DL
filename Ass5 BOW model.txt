Ass5 BOW model

import re
import numpy as np
import string
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline

from subprocess import check_output
from wordcloud import WordCloud, STOPWORDS

stopwords = set(STOPWORDS)
sentences ="""We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells."""

wordcloud = WordCloud(
                          background_color='white',
                          stopwords=stopwords,
                          max_words=200,
                          max_font_size=40,
                          random_state=42
                         ).generate(sentences)

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))
axes.imshow(wordcloud)
axes.axis('off')
fig.tight_layout()

#
sentences = """We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells."""

#
# remove special characters
sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)

# remove 1 letter words
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip()

# lower all characters
sentences = sentences.lower()
print(sentences)

#words = sentences.split()
vocab = set(words)
print(words)
print(vocab)

#vocab_size = len(vocab)
embed_dim = 10
context_size = 2

word_to_ix = {word: i for i, word in enumerate(vocab)}
ix_to_word = {i: word for i, word in enumerate(vocab)}
print(word_to_ix)
print(ix_to_word)

#data = []
for i in range(2, len(words) - 2):
    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]
    target = words[i]
    data.append((context, target))
print(data[:5])

embeddings =  np.random.random_sample((vocab_size, embed_dim))
print(embeddings)

# -------------------------------
# Step 3: Generate training matrices
# -------------------------------

def make_one_hot(index, size):
    vec = np.zeros(size)
    vec[index] = 1
    return vec

X_train = []
Y_train = []

for context, target in data:
    context_indices = [word_to_ix[w] for w in context]
    target_index = word_to_ix[target]

    # context embeddings averaged (CBOW)
    X_train.append(np.mean([embeddings[i] for i in context_indices], axis=0))
    # one hot for target
    Y_train.append(make_one_hot(target_index, vocab_size))

X_train = np.array(X_train)
Y_train = np.array(Y_train)

# Step 4: Train CBOW Model
# -------------------------------

learning_rate = 0.01
epochs = 2000

for epoch in range(epochs):
    # forward pass
    y_pred = np.dot(X_train, embeddings.T)
    y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)

    # loss
    loss = -np.sum(Y_train * np.log(y_pred + 1e-9))

    # backward pass
    error = y_pred - Y_train
    grad = np.dot(error.T, X_train)
    embeddings -= learning_rate * grad

    if epoch % 400 == 0:
        print(f"Epoch {epoch} Loss: {loss}")

# Step 5: Check Similar Words
# -------------------------------

def similar_words(word):
    word_vec = embeddings[word_to_ix[word]]
    sims = {}
    for w in vocab:
        vec = embeddings[word_to_ix[w]]
        sim = np.dot(word_vec, vec) / (np.linalg.norm(word_vec) * np.linalg.norm(vec))
        sims[w] = sim
    return sorted(sims.items(), key=lambda x: x[1], reverse=True)

print("\nWords similar to 'process':")
print(similar_words("process")[:10])

